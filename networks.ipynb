{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "from tqdm import tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(186)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "MAX_SENTENCE_LENGTH = 1000\n",
    "EMBED_SIZE = 300\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "label_to_id = {'contradiction' : 0, 'entailment' : 1, 'neutral' : 2}\n",
    "\n",
    "def load_fasttext():\n",
    "    fasttext_home = './'\n",
    "    words_to_load = 50000\n",
    "\n",
    "    loaded_embeddings = np.zeros((words_to_load + 2, EMBED_SIZE)) #+2 to account for pad and unk tokens\n",
    "    words = {}\n",
    "    idx2words = {}\n",
    "    with open(fasttext_home + 'wiki-news-300d-1M.vec') as f:\n",
    "        loaded_embeddings[PAD_IDX, :] = np.zeros((1, EMBED_SIZE))\n",
    "        loaded_embeddings[UNK_IDX, :] = np.zeros((1, EMBED_SIZE))\n",
    "        words[PAD] = PAD_IDX\n",
    "        words[UNK] = UNK_IDX\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            idx = i + 2 #+2 to account for PAD and UNK tokens\n",
    "            loaded_embeddings[idx, :] = np.asarray(s[1:])\n",
    "            words[s[0]] = idx \n",
    "            idx2words[idx] = s[0]\n",
    "    return loaded_embeddings, words, idx2words\n",
    "\n",
    "def load_snli_data():\n",
    "    snli_train = pd.read_csv('./snli_train.tsv', names=['sentence1', 'sentence2', 'label'], skiprows=1, sep='\\t|\\n', engine='python')\n",
    "    snli_val = pd.read_csv('./snli_val.tsv', names=['sentence1', 'sentence2', 'label'], skiprows=1, sep='\\t|\\n', engine='python')\n",
    "    return snli_train[['sentence1', 'sentence2']], [label_to_id[x] for x in snli_train['label']], snli_val[['sentence1', 'sentence2']], [label_to_id[x] for x in snli_val['label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings, words, idx2words = load_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_sentences, snli_train_labels, snli_val_sentences, snli_val_labels = load_snli_data()\n",
    "def map_sentence_to_idxs(sentence):\n",
    "    return [words[tkn] if tkn in words else UNK_IDX for tkn in sentence]\n",
    "snli_train_sentences_idxs = snli_train_sentences.applymap(lambda x: map_sentence_to_idxs(x))\n",
    "snli_val_sentences_idxs = snli_val_sentences.applymap(lambda x: map_sentence_to_idxs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: two sentence lists \n",
    "        @param target_list: list of targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.x1 = data_list['sentence1']\n",
    "        self.x2 = data_list['sentence2']\n",
    "        self.y = target_list\n",
    "        assert (len(self.x1) == len(self.x2) == len(self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        \n",
    "    def __getitem__(self, key):        \n",
    "        sent1_idx = self.x1[key][:MAX_SENTENCE_LENGTH]\n",
    "        sent2_idx = self.x2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.y[key]\n",
    "        return [sent1_idx, sent2_idx, len(sent1_idx), len(sent2_idx), label]\n",
    "\n",
    "def SNLI_collate_function(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sentence1_list = []\n",
    "    sentence2_list = []\n",
    "    label_list = []\n",
    "    length_list_1 = []\n",
    "    length_list_2 = []\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        length_list_1.append(datum[2])\n",
    "        length_list_2.append(datum[3])\n",
    "        label_list.append(datum[4])\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence1_list.append(padded_vec_1)\n",
    "        padded_vec_2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sentence2_list.append(padded_vec_2)\n",
    "    return [torch.from_numpy(np.array(sentence1_list)), torch.from_numpy(np.array(sentence2_list)), \n",
    "#             torch.cuda.LongTensor(length_list_1), torch.cuda.LongTensor(length_list_2), \n",
    "            torch.cuda.LongTensor(label_list)]\n",
    "#             torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_dataset = SNLIDataset(snli_train_sentences_idxs, snli_train_labels)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_function,\n",
    "                                           shuffle=True)\n",
    "snli_val_dataset = SNLIDataset(snli_val_sentences_idxs, snli_val_labels)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_function,\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        # BidirectionalGRU Accepts the following hyperparams:\n",
    "        # hidden_size: Hidden Size of layer in the GRU\n",
    "        # num_layers: number of layers in the GRU\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(loaded_embeddings), freeze=False)\n",
    "        self.gru = nn.GRU(EMBED_SIZE, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        #update embedding if token is unk\n",
    "        m = (x == 1).type(torch.cuda.FloatTensor)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        gru_out, hidden = self.gru(embed.cuda(), self.hidden.cuda())\n",
    "        return gru_out, hidden[0, :, :] + hidden[1, :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n",
      "tensor([ 0.1073,  0.0089,  0.0006,  0.0055, -0.0646, -0.0600,  0.0450, -0.0133,\n",
      "        -0.0357,  0.0430, -0.0356, -0.0032,  0.0073, -0.0001,  0.0258, -0.0166,\n",
      "         0.0075,  0.0686,  0.0392,  0.0753,  0.0115, -0.0087,  0.0421,  0.0265,\n",
      "        -0.0601,  0.2420,  0.0199, -0.0739, -0.0031, -0.0263, -0.0062,  0.0168,\n",
      "        -0.0357, -0.0249,  0.0190, -0.0184, -0.0537,  0.1420,  0.0600,  0.0226,\n",
      "        -0.0038, -0.0675, -0.0036, -0.0080,  0.0570,  0.0208,  0.0223, -0.0256,\n",
      "        -0.0153,  0.0022, -0.0482,  0.0131, -0.6016, -0.0088,  0.0106,  0.0229,\n",
      "         0.0336,  0.0071,  0.0887,  0.0237, -0.0290, -0.0405, -0.0125,  0.0147,\n",
      "         0.0475,  0.0647,  0.0474,  0.0199,  0.0408,  0.0322,  0.0036,  0.0350,\n",
      "        -0.0723, -0.0305,  0.0184, -0.0026,  0.0240, -0.0160, -0.0308,  0.0434,\n",
      "         0.0147, -0.0457, -0.0267, -0.1703, -0.0099,  0.0417,  0.0235, -0.0260,\n",
      "        -0.1519, -0.0116, -0.0306, -0.0413,  0.0330,  0.0723,  0.0365, -0.0001,\n",
      "         0.0042,  0.0346,  0.0277, -0.0305,  0.0784, -0.0404,  0.0187, -0.0225,\n",
      "        -0.0206, -0.0179, -0.2428,  0.0669,  0.0523,  0.0527,  0.0149, -0.0708,\n",
      "        -0.0987,  0.0263, -0.0611,  0.0302,  0.0216,  0.0313, -0.0140, -0.2495,\n",
      "        -0.0346, -0.0480,  0.0250,  0.2130, -0.0330, -0.1553, -0.0292, -0.0346,\n",
      "         0.1074,  0.0010, -0.0117, -0.0057, -0.1280, -0.0038,  0.0130, -0.1157,\n",
      "        -0.0108,  0.0275,  0.0158, -0.0169,  0.0070,  0.0247,  0.0510,  1.0292,\n",
      "        -0.0283, -0.0310, -0.0026, -0.0343,  0.0578,  0.0444,  0.0812, -0.0211,\n",
      "        -0.0872,  0.0169,  0.0499,  0.0485,  0.0227, -0.0323, -0.0035,  0.0435,\n",
      "        -0.0275,  0.0154,  0.0135, -0.0484, -0.0699, -0.0502,  0.2745, -0.0003,\n",
      "        -0.0371,  0.0517, -0.0908,  0.0013,  0.0360,  0.0280,  0.0839,  0.0980,\n",
      "        -0.0490, -0.2423, -0.0142,  0.0024, -0.0207,  0.0012,  0.0088, -0.0143,\n",
      "        -0.0197,  0.0515, -0.0085,  0.0257,  0.2154,  0.0301,  0.0211,  0.0530,\n",
      "        -0.0005,  0.0177,  0.0016, -0.0053, -0.0162, -0.0223, -0.1862,  0.0398,\n",
      "         0.0658, -0.0962, -0.0076, -0.0075, -0.0342, -0.0265,  0.0420,  0.0522,\n",
      "        -0.0266,  0.0201, -0.1331, -0.0367,  0.0351,  0.0518, -0.0087,  0.0599,\n",
      "        -0.1086, -0.0188,  0.0481,  0.0105, -0.0060,  0.0151, -0.0031,  0.0077,\n",
      "        -0.0276, -0.0373, -0.0203,  0.0472,  0.0246,  0.1440,  0.0542, -0.0225,\n",
      "         0.2495,  0.1617,  0.0038,  0.1119, -0.0230, -0.0785,  0.0250, -0.0616,\n",
      "        -0.0485,  0.0225,  0.0281,  0.0041,  0.0112,  0.0172,  0.0291, -0.0282,\n",
      "         0.0026,  0.4055,  0.0392,  0.0088,  0.0228,  0.0299,  0.1195,  0.0545,\n",
      "        -0.0020,  0.0020,  0.0490,  0.0145, -0.0086,  0.0098, -0.0236,  0.0171,\n",
      "        -0.0765, -0.0400,  0.0128,  0.0011,  0.0042,  0.0244,  0.0075,  0.0200,\n",
      "         0.0201,  0.0196, -0.0377, -0.0432, -0.0073, -0.0021,  0.0183,  0.0076,\n",
      "         0.1805, -0.0551,  0.0075, -0.0516,  0.0420, -0.0068, -0.0711, -0.1408,\n",
      "         0.0504,  0.0276,  0.0470,  0.0323, -0.0219,  0.0010,  0.0089,  0.0276,\n",
      "         0.0186,  0.0050,  0.1173, -0.0400], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-05f0470a317a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mencoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-d2f340e5f1bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgru_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgru_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_gru_model(loader, gru_model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    gru_model.eval()\n",
    "    for sent1_batch, sent2_batch, labels_batch in loader:\n",
    "        _, hidden_1 = gru_model(sent1_batch.cuda())\n",
    "        _, hidden_2 = gru_model(sent2_batch.cuda())\n",
    "        encoded_output = torch.cat([hidden_1, hidden_2], dim=1).cuda()\n",
    "        outputs = []\n",
    "        for output in encoded_output:\n",
    "            outputs.append(fully_connected(output.cuda()))\n",
    "        outputs = torch.stack(outputs).cuda()\n",
    "        predicted = F.softmax(outputs)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels_batch.size(0)\n",
    "        correct += predicted.eq(labels_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "del gru_model\n",
    "torch.cuda.empty_cache()\n",
    "HIDDEN_SIZE = 200\n",
    "gru_model = BidirectionalGRU(hidden_size=HIDDEN_SIZE, num_layers=1)\n",
    "gru_model = gru_model.cuda()\n",
    "\n",
    "num_epochs = 10 # number epoch to train\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, gru_model.parameters()), lr=learning_rate)\n",
    "fully_connected = nn.Sequential(nn.Linear(HIDDEN_SIZE*2, HIDDEN_SIZE), nn.ReLU(inplace=True), nn.Linear(HIDDEN_SIZE, 3)).cuda()\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, (sent1, sent2, labels) in enumerate(snli_train_loader):\n",
    "        gru_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        _, hidden_1 = gru_model(sent1.cuda())\n",
    "        _, hidden_2 = gru_model(sent2.cuda())\n",
    "        encoded_output = torch.cat([hidden_1, hidden_2], dim=1).cuda()\n",
    "        outputs = []\n",
    "        for output in encoded_output:\n",
    "            outputs.append(fully_connected(output.cuda()))\n",
    "        outputs = torch.stack(outputs).cuda()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i > 0 and i % 100 == 0):\n",
    "            val_acc = test_gru_model(snli_val_loader, gru_model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(snli_train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/1563], Validation Acc: 35.6\n",
      "Epoch: [1/10], Step: [201/1563], Validation Acc: 35.8\n",
      "Epoch: [1/10], Step: [301/1563], Validation Acc: 37.3\n",
      "Epoch: [1/10], Step: [401/1563], Validation Acc: 39.2\n",
      "Epoch: [1/10], Step: [501/1563], Validation Acc: 37.6\n",
      "Epoch: [1/10], Step: [601/1563], Validation Acc: 41.3\n",
      "Epoch: [1/10], Step: [701/1563], Validation Acc: 41.2\n",
      "Epoch: [1/10], Step: [801/1563], Validation Acc: 41.8\n",
      "Epoch: [1/10], Step: [901/1563], Validation Acc: 42.3\n",
      "Epoch: [1/10], Step: [1001/1563], Validation Acc: 44.6\n",
      "Epoch: [1/10], Step: [1101/1563], Validation Acc: 43.6\n",
      "Epoch: [1/10], Step: [1201/1563], Validation Acc: 45.5\n",
      "Epoch: [1/10], Step: [1301/1563], Validation Acc: 45.6\n",
      "Epoch: [1/10], Step: [1401/1563], Validation Acc: 43.5\n",
      "Epoch: [1/10], Step: [1501/1563], Validation Acc: 44.7\n",
      "Epoch: [2/10], Step: [101/1563], Validation Acc: 48.0\n",
      "Epoch: [2/10], Step: [201/1563], Validation Acc: 46.7\n",
      "Epoch: [2/10], Step: [301/1563], Validation Acc: 45.8\n",
      "Epoch: [2/10], Step: [401/1563], Validation Acc: 49.6\n",
      "Epoch: [2/10], Step: [501/1563], Validation Acc: 47.7\n",
      "Epoch: [2/10], Step: [601/1563], Validation Acc: 48.0\n",
      "Epoch: [2/10], Step: [701/1563], Validation Acc: 47.9\n",
      "Epoch: [2/10], Step: [801/1563], Validation Acc: 49.8\n",
      "Epoch: [2/10], Step: [901/1563], Validation Acc: 49.3\n",
      "Epoch: [2/10], Step: [1001/1563], Validation Acc: 49.6\n",
      "Epoch: [2/10], Step: [1101/1563], Validation Acc: 49.2\n",
      "Epoch: [2/10], Step: [1201/1563], Validation Acc: 51.2\n",
      "Epoch: [2/10], Step: [1301/1563], Validation Acc: 51.5\n",
      "Epoch: [2/10], Step: [1401/1563], Validation Acc: 51.6\n",
      "Epoch: [2/10], Step: [1501/1563], Validation Acc: 51.0\n",
      "Epoch: [3/10], Step: [101/1563], Validation Acc: 51.5\n",
      "Epoch: [3/10], Step: [201/1563], Validation Acc: 51.9\n",
      "Epoch: [3/10], Step: [301/1563], Validation Acc: 53.3\n",
      "Epoch: [3/10], Step: [401/1563], Validation Acc: 51.7\n",
      "Epoch: [3/10], Step: [501/1563], Validation Acc: 53.4\n",
      "Epoch: [3/10], Step: [601/1563], Validation Acc: 51.5\n",
      "Epoch: [3/10], Step: [701/1563], Validation Acc: 51.6\n",
      "Epoch: [3/10], Step: [801/1563], Validation Acc: 40.7\n",
      "Epoch: [3/10], Step: [901/1563], Validation Acc: 42.4\n",
      "Epoch: [3/10], Step: [1001/1563], Validation Acc: 43.9\n",
      "Epoch: [3/10], Step: [1101/1563], Validation Acc: 47.0\n",
      "Epoch: [3/10], Step: [1201/1563], Validation Acc: 48.5\n",
      "Epoch: [3/10], Step: [1301/1563], Validation Acc: 51.0\n",
      "Epoch: [3/10], Step: [1401/1563], Validation Acc: 50.0\n",
      "Epoch: [3/10], Step: [1501/1563], Validation Acc: 53.4\n",
      "Epoch: [4/10], Step: [101/1563], Validation Acc: 52.7\n",
      "Epoch: [4/10], Step: [201/1563], Validation Acc: 52.6\n",
      "Epoch: [4/10], Step: [301/1563], Validation Acc: 52.9\n",
      "Epoch: [4/10], Step: [401/1563], Validation Acc: 52.7\n",
      "Epoch: [4/10], Step: [501/1563], Validation Acc: 39.1\n",
      "Epoch: [4/10], Step: [601/1563], Validation Acc: 48.1\n",
      "Epoch: [4/10], Step: [701/1563], Validation Acc: 46.5\n",
      "Epoch: [4/10], Step: [801/1563], Validation Acc: 47.0\n",
      "Epoch: [4/10], Step: [901/1563], Validation Acc: 49.6\n",
      "Epoch: [4/10], Step: [1001/1563], Validation Acc: 50.0\n",
      "Epoch: [4/10], Step: [1101/1563], Validation Acc: 50.4\n",
      "Epoch: [4/10], Step: [1201/1563], Validation Acc: 50.3\n",
      "Epoch: [4/10], Step: [1301/1563], Validation Acc: 51.7\n",
      "Epoch: [4/10], Step: [1401/1563], Validation Acc: 52.7\n",
      "Epoch: [4/10], Step: [1501/1563], Validation Acc: 50.8\n",
      "Epoch: [5/10], Step: [101/1563], Validation Acc: 50.0\n",
      "Epoch: [5/10], Step: [201/1563], Validation Acc: 51.9\n",
      "Epoch: [5/10], Step: [301/1563], Validation Acc: 50.0\n",
      "Epoch: [5/10], Step: [401/1563], Validation Acc: 51.4\n",
      "Epoch: [5/10], Step: [501/1563], Validation Acc: 51.7\n",
      "Epoch: [5/10], Step: [601/1563], Validation Acc: 52.5\n",
      "Epoch: [5/10], Step: [701/1563], Validation Acc: 52.4\n",
      "Epoch: [5/10], Step: [801/1563], Validation Acc: 52.7\n",
      "Epoch: [5/10], Step: [901/1563], Validation Acc: 51.8\n",
      "Epoch: [5/10], Step: [1001/1563], Validation Acc: 50.4\n",
      "Epoch: [5/10], Step: [1101/1563], Validation Acc: 51.7\n",
      "Epoch: [5/10], Step: [1201/1563], Validation Acc: 53.1\n",
      "Epoch: [5/10], Step: [1301/1563], Validation Acc: 53.3\n",
      "Epoch: [5/10], Step: [1401/1563], Validation Acc: 52.8\n",
      "Epoch: [5/10], Step: [1501/1563], Validation Acc: 52.5\n",
      "Epoch: [6/10], Step: [101/1563], Validation Acc: 52.1\n",
      "Epoch: [6/10], Step: [201/1563], Validation Acc: 52.9\n",
      "Epoch: [6/10], Step: [301/1563], Validation Acc: 52.6\n",
      "Epoch: [6/10], Step: [401/1563], Validation Acc: 53.8\n",
      "Epoch: [6/10], Step: [501/1563], Validation Acc: 53.9\n",
      "Epoch: [6/10], Step: [601/1563], Validation Acc: 53.7\n",
      "Epoch: [6/10], Step: [701/1563], Validation Acc: 53.6\n",
      "Epoch: [6/10], Step: [801/1563], Validation Acc: 54.0\n",
      "Epoch: [6/10], Step: [901/1563], Validation Acc: 52.6\n",
      "Epoch: [6/10], Step: [1001/1563], Validation Acc: 53.9\n",
      "Epoch: [6/10], Step: [1101/1563], Validation Acc: 52.3\n",
      "Epoch: [6/10], Step: [1201/1563], Validation Acc: 53.4\n",
      "Epoch: [6/10], Step: [1301/1563], Validation Acc: 53.9\n",
      "Epoch: [6/10], Step: [1401/1563], Validation Acc: 54.2\n",
      "Epoch: [6/10], Step: [1501/1563], Validation Acc: 54.3\n",
      "Epoch: [7/10], Step: [101/1563], Validation Acc: 54.2\n",
      "Epoch: [7/10], Step: [201/1563], Validation Acc: 54.2\n",
      "Epoch: [7/10], Step: [301/1563], Validation Acc: 54.2\n",
      "Epoch: [7/10], Step: [401/1563], Validation Acc: 54.6\n",
      "Epoch: [7/10], Step: [501/1563], Validation Acc: 54.6\n",
      "Epoch: [7/10], Step: [601/1563], Validation Acc: 54.5\n",
      "Epoch: [7/10], Step: [701/1563], Validation Acc: 54.7\n",
      "Epoch: [7/10], Step: [801/1563], Validation Acc: 55.9\n",
      "Epoch: [7/10], Step: [901/1563], Validation Acc: 54.4\n",
      "Epoch: [7/10], Step: [1001/1563], Validation Acc: 53.9\n",
      "Epoch: [7/10], Step: [1101/1563], Validation Acc: 55.7\n",
      "Epoch: [7/10], Step: [1201/1563], Validation Acc: 56.1\n",
      "Epoch: [7/10], Step: [1301/1563], Validation Acc: 56.5\n",
      "Epoch: [7/10], Step: [1401/1563], Validation Acc: 54.9\n",
      "Epoch: [7/10], Step: [1501/1563], Validation Acc: 56.0\n",
      "Epoch: [8/10], Step: [101/1563], Validation Acc: 56.0\n",
      "Epoch: [8/10], Step: [201/1563], Validation Acc: 55.1\n",
      "Epoch: [8/10], Step: [301/1563], Validation Acc: 56.8\n",
      "Epoch: [8/10], Step: [401/1563], Validation Acc: 56.0\n",
      "Epoch: [8/10], Step: [501/1563], Validation Acc: 56.1\n",
      "Epoch: [8/10], Step: [601/1563], Validation Acc: 54.9\n",
      "Epoch: [8/10], Step: [701/1563], Validation Acc: 57.5\n",
      "Epoch: [8/10], Step: [801/1563], Validation Acc: 57.0\n",
      "Epoch: [8/10], Step: [901/1563], Validation Acc: 56.6\n",
      "Epoch: [8/10], Step: [1001/1563], Validation Acc: 54.0\n",
      "Epoch: [8/10], Step: [1101/1563], Validation Acc: 56.5\n",
      "Epoch: [8/10], Step: [1201/1563], Validation Acc: 57.8\n",
      "Epoch: [8/10], Step: [1301/1563], Validation Acc: 57.1\n",
      "Epoch: [8/10], Step: [1401/1563], Validation Acc: 57.6\n",
      "Epoch: [8/10], Step: [1501/1563], Validation Acc: 55.8\n",
      "Epoch: [9/10], Step: [101/1563], Validation Acc: 57.4\n",
      "Epoch: [9/10], Step: [201/1563], Validation Acc: 58.5\n",
      "Epoch: [9/10], Step: [301/1563], Validation Acc: 58.2\n",
      "Epoch: [9/10], Step: [401/1563], Validation Acc: 58.3\n",
      "Epoch: [9/10], Step: [501/1563], Validation Acc: 58.2\n",
      "Epoch: [9/10], Step: [601/1563], Validation Acc: 57.1\n",
      "Epoch: [9/10], Step: [701/1563], Validation Acc: 58.2\n",
      "Epoch: [9/10], Step: [801/1563], Validation Acc: 58.2\n",
      "Epoch: [9/10], Step: [901/1563], Validation Acc: 57.5\n",
      "Epoch: [9/10], Step: [1001/1563], Validation Acc: 54.0\n",
      "Epoch: [9/10], Step: [1101/1563], Validation Acc: 57.6\n",
      "Epoch: [9/10], Step: [1201/1563], Validation Acc: 55.9\n",
      "Epoch: [9/10], Step: [1301/1563], Validation Acc: 56.9\n",
      "Epoch: [9/10], Step: [1401/1563], Validation Acc: 58.1\n",
      "Epoch: [9/10], Step: [1501/1563], Validation Acc: 57.3\n",
      "Epoch: [10/10], Step: [101/1563], Validation Acc: 57.4\n",
      "Epoch: [10/10], Step: [201/1563], Validation Acc: 57.4\n",
      "Epoch: [10/10], Step: [301/1563], Validation Acc: 57.5\n",
      "Epoch: [10/10], Step: [401/1563], Validation Acc: 57.8\n",
      "Epoch: [10/10], Step: [501/1563], Validation Acc: 57.9\n",
      "Epoch: [10/10], Step: [601/1563], Validation Acc: 58.9\n",
      "Epoch: [10/10], Step: [701/1563], Validation Acc: 59.0\n",
      "Epoch: [10/10], Step: [801/1563], Validation Acc: 59.1\n",
      "Epoch: [10/10], Step: [901/1563], Validation Acc: 58.9\n",
      "Epoch: [10/10], Step: [1001/1563], Validation Acc: 57.9\n",
      "Epoch: [10/10], Step: [1101/1563], Validation Acc: 58.9\n",
      "Epoch: [10/10], Step: [1201/1563], Validation Acc: 58.6\n",
      "Epoch: [10/10], Step: [1301/1563], Validation Acc: 60.3\n",
      "Epoch: [10/10], Step: [1401/1563], Validation Acc: 60.5\n",
      "Epoch: [10/10], Step: [1501/1563], Validation Acc: 59.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
